\documentclass[12pt]{article}
\usepackage[pdftex]{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\pagestyle{empty}
\author{Chris Camano: ccamano@sfsu.edu}
\title{MATH 425  Homework 3 }
\date{4/4/2022}

\topmargin -0.6in
\headsep 0.40in
\oddsidemargin 0.0in
\textheight 9.0in
\textwidth 6.5in

\newcommand{\econst}{\mathrm{e}}
\newcommand{\diff}{\mathrm{d}}
\newcommand{\dwrt}[1]{\frac{\diff}{\diff #1}}
%%%%%%Macros for 425%%%%%%%%%%%%%%%%%%%
\newcommand{\q}{\quad}
\newcommand{\tab}{\\\\}
\renewcommand{\labelenumi}{\alph{enumi})}
\newcommand{\sect}[1]{\section*{#1}}

%%%%%%Vector Spaces%%%%%%%%%%%%%%%%%%%
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\rtwo}{\mathbb{R}^2}
\newcommand{\mxn}{{mxn}}

%%%%%%Sets and common phrases%%%%%%%%%
\newcommand{\Axb}{\textbf{Ax=b} }
\newcommand{\Axz}{\textbf{Ax=0} }
\newcommand{\dim}{\text{dim}}
\newcommand{\lc}{linear combination }
\newcommand{\let}{\text{Let }}
\newcommand{\tf}{\therefore}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\everymath={\displaystyle}


\begin{document}
\maketitle
\sect{Problem 1}
\begin{proof}
  Prove that the set$\{(x_1, x_2,0):x_1, x_2\in\F\}$is a subspace of$\F^3$\\
  Let V denote the space defined by $\{(x_1, x_2,0):x_1, x_2\in\F\}$. In order for V to be a subspace it must contain the zero vector, be closed under addition, and closed under multiplication.
  \begin{itemize}
    \item The vector space V contains the zero vector when $x_1$ and $x_2$ are both zero
    \item given some $u,v \in V$ u+v= $(u_1,u_2,0)+(v_1,v_2,0)=(u_1+v_1,u_2+v_2,0)$  Here it can be seen that $\forall u, v \in V$ that $u+v \in V$ as $\nexists u,v$ such that u+v $\notin  V$
    \item Finally, let c$\in \F$ $\forall u \in V$ $c \cdot u= c( u_1,u_2,0)=(cu_1,cu_2,0)$ thus it is shown that the vectors of V are closed under scalar multiplication
  \end{itemize}
  With these three characteristics in consideration it is clear that V is a subspace as it contains the zero vector, is closed under addition and scalar multiplication. One may also recall the geometric intution that the subspace V is indeed the two dimensional plane embedded in three dimensions.
\end{proof}
\sect{Problem 2}
\begin{proof}
  \[
  \text{Let A =}
    \begin{bmatrix}
      1 &-2 &0 & 3\\
      2& -3 &-1 & -4\\
      3 & -5 & -1 & -1\\
    \end{bmatrix}
  \]
  Find a basis for Row A and NulA\\
  Find the inner product of each vector in the basis of Row A with each vector in the basis of NulA. \\
  \begin{itemize}
    \item Find a basis for RowA and NulA
    \begin{align*}
      &  \begin{bmatrix}
          1 &-2 &0 & 3\\
          2& -3 &-1 & -4\\
          3 & -5 & -1 & -1\\
        \end{bmatrix} \sim \begin{bmatrix}
            1 &0 &-2 & -17\\
            0& 1 &-1 & -10\\
            0 & 0 & 0 & 0\\
          \end{bmatrix} \\
        &\text{RowA}=\Bigg\{\begin{bmatrix}
          1\\0\\-2\\-17
      \end{bmatrix}
      ,\begin{bmatrix}
        0\\1\\-1\\-10
       \end{bmatrix}\Bigg\}\\
      &\text{NulA}=\Bigg\{\begin{bmatrix}
      2\\1\\1\\0
     \end{bmatrix}
     ,\begin{bmatrix}
      17\\10\\0\\1
    \end{bmatrix}\Bigg\}
    \end{align*}
    \item Find the inner product of each vector in the basis of Row A with each vector in the basis of NulA.
    \[
    \begin{bmatrix}

      1\\0\\-2\\-17
    \end{bmatrix}\cdot
    \begin{bmatrix}
      2\\1\\1\\0
    \end{bmatrix}=0\quad
    \begin{bmatrix}
      0\\1\\-1\\-10
    \end{bmatrix}\cdot\begin{bmatrix}
      17\\10\\0\\1
  \end{bmatrix}=0
    \]
  \end{itemize}
\end{proof}
\sect{Problem 3}
\begin{proof}$
 Letu_1=\begin{bmatrix}
   3\\-3\\0
 \end{bmatrix},u_2=\begin{bmatrix}
   2\\2\\-1
 \end{bmatrix},u_3=\begin{bmatrix}
   1\\1\\4
 \end{bmatrix},$ and $x=\begin{bmatrix}
   5\\-3\\1
 \end{bmatrix}.$  Show that$\{u_1,u_2,u_3\}$is an orthogonal basis for R3 then express x as a linear combination of $u_1,u_2$and$u_3$
 Let A be the matrix formed by the vectors $u_1,u_2$and$u_3$
 \[
   A =\begin{bmatrix}
     3&2&1\\-3&2&1\\0&-1&4
 \end{bmatrix}
 \sim \begin{bmatrix}
   1&0&0\\0&1&0\\0&0&1
 \end{bmatrix}
 \]
 By the inveritble matrix theorem since the columns of A are row reducible to the identity matrix the matrix A is linearly independent. Next to prove orthogonality consider the inner product of the three vectors in their unique configurations.
 \begin{itemize}
   \item $u_1 \cdot u_2$=0
   \item $u_2 \cdot u_3$=0
   \item $u_2 \cdot u_3$=0
 \end{itemize}
 ( I did not write out the vectors for u1,u2,u3 here to save time with latex hope that okay!)
 Since A is linearly independent and the columns of A are orthogonal to one another the matrix A is a orthogonal basis for $\R^3$. Solving for X:
 \[
 [A x] =\begin{bmatrix}
   3&2&1&5\\-3&2&1&-3\\0&-1&4&1
 \end{bmatrix}\sim \begin{bmatrix}
  1&0&0&\frac{4}{3}\\0&1&0&\frac{1}{3}\\0&0&1&\frac{1}{3}
 \end{bmatrix}
 \]
 \[
   x=\frac{4}{3}u_1+\frac{1}{3}u_2+\frac{1}{3}u_3
 \]
\end{proof}
\sect{Problem 4}
\begin{proof}
  Suppose W is a subspace of $\R^n$ spanned by n nonzero orthogonal vectors.  Explain why W=$\R^n$.\\
  Firstly since W is a subspace we know it contains the zero vector is closed under addition and is closed under sclar multiplication. Next orthogonal vectors must be lineary independent. Since we have n linearly independent vectors in $\R^n$ we can then say that W Spans $\R^n$ as the vectors in W form an orthogonal basis for an n dimensional space which just so happens to be the argument we are claiming.
\end{proof}
\sect{Problem 5}
\begin{proof}
  Refer to homework assignment for full problem description:\\
  Let W = the matrix formed by the columns of its span.
  \[
    W =\begin{bmatrix}
      1&1&0\\1&0&-1\\0&1&1\\-1&1&-1
  \end{bmatrix}
  \]
  To write the vector y as a linear combination of the vectors in W we must use the orthogonal decomposition theorem. :
  \[
   \hat{y}=\frac{y \cdot u_1}{u_1 \cdot u_1}u_1+...+\frac{y \cdot u_p}{u_p \cdot u_p}u_p
  \]
  given p orthogonal vectors.\\\\\\
  \[
    \hat{y}=
    \frac{  \begin{bmatrix}
        3\\4\\5\\6
      \end{bmatrix}\cdot \begin{bmatrix}
        1\\1\\0\\-1
    \end{bmatrix}}{\begin{bmatrix}
      1\\1\\0\\-1
    \end{bmatrix} \cdot \begin{bmatrix}
      1\\1\\0\\-1
    \end{bmatrix}}\begin{bmatrix}
      1\\1\\0\\-1
    \end{bmatrix}
    +
    \frac{  \begin{bmatrix}
        3\\4\\5\\6
      \end{bmatrix}\cdot \begin{bmatrix}
        1\\0\\1\\1
    \end{bmatrix}}{\begin{bmatrix}
      1\\0\\1\\1
    \end{bmatrix} \cdot \begin{bmatrix}
      1\\0\\1\\1
    \end{bmatrix}}\begin{bmatrix}
      1\\0\\1\\1
    \end{bmatrix}+
  \]
  \[
    \frac{  \begin{bmatrix}
        3\\4\\5\\6
      \end{bmatrix}\cdot \begin{bmatrix}
        0\\-1\\1\\-1
    \end{bmatrix}}{\begin{bmatrix}
        0\\-1\\1\\-1
    \end{bmatrix} \cdot \begin{bmatrix}
        0\\-1\\1\\-1
    \end{bmatrix}}\begin{bmatrix}
        0\\-1\\1\\-1
    \end{bmatrix}=\begin{bmatrix}
      5\\2\\3\\6
    \end{bmatrix} \quad z=y-\hat{y}=\begin{bmatrix}
        3\\4\\5\\6
      \end{bmatrix}-\begin{bmatrix}
        5\\2\\3\\6
      \end{bmatrix}=\begin{bmatrix}
        -2\\2\\2\\0
    \end{bmatrix}
  \]
  $z\in W^\perp$ \therefore$
  \[$
    y=\begin{bmatrix}
      5\\2\\3\\6
    \end{bmatrix}+\begin{bmatrix}
      -2\\2\\2\\0
  \end{bmatrix}
  \]
\end{proof}
\sect{Problem 6}
\begin{proof}
  Find the best approximation to z by vectors of the form $c_1v_1+c_2v_2$
  \[
    \begin{bmatrix}
      2\\4\\0\\-1
    \end{bmatrix}=\begin{bmatrix}
      2&5\\0&-2\\-1&4\\-3&2
  \end{bmatrix}\begin{bmatrix}
    c_1\\c_2
  \end{bmatrix}
  \]
  This is now a least squares regression problem of the form:
  \[
    A^TAx=A^Tb
  \]
  when we multiply by the transpose on both sides.
  \[\begin{bmatrix}
    2&5\\0&-2\\-1&4\\-3&2
  \end{bmatrix}^T\begin{bmatrix}
    2&5\\0&-2\\-1&4\\-3&2
  \end{bmatrix}\begin{bmatrix}
  c_1\\c_2
  \end{bmatrix}=\begin{bmatrix}
    2&5\\0&-2\\-1&4\\-3&2
  \end{bmatrix}^T
  \begin{bmatrix}
    2\\4\\0\\-1
  \end{bmatrix}
  \]
  \[
    \begin{bmatrix}
      14&0\\0&49
    \end{bmatrix}\begin{bmatrix}
    c_1\\c_2
  \end{bmatrix}=\begin{bmatrix}
      7\\0
  \end{bmatrix}
  \]
  \[
    \begin{bmatrix}
      14&0&7\\0&49&0
    \end{bmatrix}\sim
    \begin{bmatrix}
      1&0&\frac{1}{2}\\
      0&1&0
    \end{bmatrix}

  \]
  $$\frac{1}{2}v_1+0v_2 \approx z$$
\end{proof}
\sect{Problem 7}
\begin{proof}
  \begin{itemize}
    \item Show that$\{u_1,u_2\}$is a basis forW= Span$\{u_1,u_2\}$.  Do not use row reduction.\\
    Let A be the matrix formed by the columns $u_1,u_2$
    \[
      A=\begin{bmatrix}
        1&5\\1&-1\\-2&2
    \end{bmatrix}
    \]
    Note that the second element of the first vector and the second element of the second vector have opposite signs. Since the first element of both vectors have the same sign there is not scalar multiple that maps $u_1$ to $u_2$ since there is not a scalar that makes some entries positive and some negative. This means that the two are linearly independent from one another Since we have 2 linearly independent vectors they form a basis for $\R^2$ or as stated in the problem Span{$u_1,u_2$}.Also Note that these vectors are orthogonal and must be linearly independent as a concequence of that relationship.
    \item Show that $u_3$ is not in W.\\
    To solve this problem consider the determinant of the matrix formed by $u_{1-3}$
    \[
      A=\begin{bmatrix}
        1&5&0\\1&-1&1\\-2&2&0
    \end{bmatrix}
    \]
    detA=-12. Since the determinant is non zero by the invertible matrix theorem the three vectrors are linearly independent. Since they are linearly independent there does not exist a linear combination of u1 and u2 that equals u3.
    \item Use the fact that u3 is not in W to construct a nonzero vector v in R3 that is orthogonal to u1 and u2\\ This vector will be found using orthogonal decomposition theorem.
    \[
      \hat{u_3}=\frac{u_3 \cdot u_1}{u_1 \cdot u_1}u_1+\frac{u_3\cdot u_2}{u_2\cdot u_2}u_2=\frac{1}{6}u_1+ \frac{-1}{30}u_2
    \]
    \[
      z=u_3-\hat{u_3}=\begin{bmatrix}
        0\\\frac{4}{5}\\\frac{2}{5}
    \end{bmatrix}
    \]
  \[
    u_1 \cdot z =0 \quad u_2 \cdot z =0
  \]
  therefore z is orthogonal to both u1 and u2
  \end{itemize}
\end{proof}
\sect{Problem 8}
\begin{proof}
  Find an orthogonal basis for the column space of the matrix A:
  \[
    \begin{bmatrix}
      -1&6&6\\3&-8&4\\1&-2&6\\1&-4&-3
    \end{bmatrix}
  \]
  For this problem I will use gramn schmidt orthogonalization to obtain an orthonormal basis for the columns of A.
  \[
    \text{Let }u_1=\begin{bmatrix}
      -1\\3\\1\\1
  \end{bmatrix}
  \]
  \[
    \text{Let }u_2=\begin{bmatrix}
      6\\-8\\-2\\-4
  \end{bmatrix}-
  \frac{  \begin{bmatrix}
    6\\-8\\-2\\-4
\end{bmatrix}\cdot \begin{bmatrix}
      -1\\3\\1\\1
  \end{bmatrix}}{\begin{bmatrix}
    -1\\3\\1\\1
\end{bmatrix} \cdot \begin{bmatrix}
    -1\\3\\1\\1
\end{bmatrix}}\begin{bmatrix}
    -1\\3\\1\\1
\end{bmatrix}=\begin{bmatrix}
  3\\1\\1\\-1
\end{bmatrix}
  \]

  \[
  \text{Let } u_3=\begin{bmatrix}
      6\\4\\6\\-3
  \end{bmatrix}-\frac{{\begin{bmatrix}
    6\\4\\6\\-3
  \end{bmatrix}\cdot  \begin{bmatrix}
      -1\\3\\1\\1
  \end{bmatrix}}}{\begin{bmatrix}
        -1\\3\\1\\1
    \end{bmatrix}\cdot \begin{bmatrix}
          -1\\3\\1\\1
      \end{bmatrix}}\begin{bmatrix}
            -1\\3\\1\\1
        \end{bmatrix}-\frac{v_3\cdot u_2}{u_2 \cdot u_2}u_2=  \renewcommand\arraystretch{2}\begin{bmatrix}
          -1\\\frac{-5}{6}\\\frac{8}{3}\\\frac{-7}{6}
      \end{bmatrix}
  \]
  Thus the set :
  \[
    \Bigg\{\begin{bmatrix}
      -1\\3\\1\\1
  \end{bmatrix},\begin{bmatrix}
    3\\1\\1\\-1
  \end{bmatrix},\renewcommand\arraystretch{2}\begin{bmatrix}
    -1\\\frac{-5}{6}\\\frac{8}{3}\\\frac{-7}{6}
\end{bmatrix}\Bigg\}
  \]
  Forms an orthogonal basis for the given vectors. To validate this claim I have checked that all three vectors in the described set are orthogonal to one another computationally.
\end{proof}
\sect{Problem 9}
\begin{proof}
Find an orthonormal basis for the columns space of the matrix A. Given how tedious it was to write the vector expansion of the gram schmidt algorithm in problem 8 I will be using a short hand for the formula and computing computationally to save time. First note that:
\[
  A=\begin{bmatrix}
    3&-3&0\\-4&14&10\\5&-7&-2
\end{bmatrix}\sim\begin{bmatrix}
  1&0&1\\0&1&1\\0&0&0
\end{bmatrix}
\]
Implying that :
\[
  \text{ColA}=\{\begin{bmatrix}
    3\\-4\\5
  \end{bmatrix},\begin{bmatrix}
    3\\-14\\-7
\end{bmatrix}\}
\]
To find an orthogonal basis I willl use the gram schmidt algorithm then normalize afterwards to obtain an orthonormal basis:
\[
  \textbf{Let }v_1=x_1=\begin{bmatrix}
    3\\-4\\5
  \end{bmatrix}
\]
\[
  \textbf{Let }v_2=x_2-\frac{x_2\cdot v_1 }{v_1 \cdot v_1}v_2=\begin{bmatrix}
    3\\6\\3
  \end{bmatrix}
\]
Thus an orthogonal basis for ColA is :
\[
  \{\begin{bmatrix}
    3\\-4\\5
  \end{bmatrix},\begin{bmatrix}
    3\\6\\3
  \end{bmatrix}
\]
normalizing:
\[
\renewcommand\arraystretch{2}
\{\begin{bmatrix}
  \frac{3\sqrt{2}}{10}\\-\frac{2\sqrt{2}}{5}\\\frac{\sqrt{2}}{2}
\end{bmatrix},\begin{bmatrix}
  \frac{\sqrt{6}}{6}\\\frac{\sqrt{6}}{3}\\\frac{\sqrt{6}}{6}
\end{bmatrix}\}
\]
\end{proof}
\sect{Problem 10}
\begin{proof}
  Let $u_1,...u_p$ be an orthogonal basis for the subspace W of $\R^n$ and let T: $
  \R^n \mapsto \R^n$ ve defined by T(x)=proj$_Wx$ Show that T is a linear transformation.
\[
  T(x)=\frac{x\cdot u_1}{||u_1||}u_1+...+\frac{x\cdot u_n}{||u_n||}u_n
\]
\[
  T(cx+y)=\frac{(cx_1+y_1)\cdot u_1}{||u_1||}u_1+....+\frac{(cx_p+y_p)\cdot u_p}{||u_p||}u_p
\]
\[
  T(cx+y)=\frac{(cx_1\cdot u_1+y_1\cdot u_1)}{||u_1||}u_1+....+\frac{(cx_p\cdot u_p+y_p\cdot u_p)}{||u_p||}u_p
\]
\[
  T(cx+y)=\frac{(cx_1\cdot u_1)}{||u_1||}u_1+\frac{y_1\cdot u_1}{||u_1||}u_1....+\frac{(cx_1\cdot u_p)}{||u_p||}u_p+\frac{y_p\cdot u_p}{||u_p||}u_p
\]
\[
  T(cx+y)=  T(cx+y)=\frac{(cx_1\cdot u_1)}{||u_1||}u_1....+\frac{(cx_1\cdot u_p)}{||u_p||}u_p+T(y)
\]
\[
  T(cx+y)=cT(x)+T(y)
\]
\end{proof}
\end{document}
